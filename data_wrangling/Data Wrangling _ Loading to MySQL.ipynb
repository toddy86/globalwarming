{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Re-run StationRecords with FULL dataset and check all cells\n",
    "- Check why there are more records in the DB than in the data object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StationDetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clean the StationDetails data\n",
    "- Enrich the data by adding the weather station location details (e.g. city, state, country etc)\n",
    "- Load the data into the MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "StationDetails = pd.read_csv(\"/Users/todddequincey/globalwarming/data/StationDetails.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Geolocation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in API key\n",
    "key = open(\"/Users/todddequincey/Desktop/api_key.txt\",\"r\")\n",
    "key = key.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libaries\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "# Get the location of each weather station\n",
    "def getlocation(lat, lon):\n",
    "    # Set return vars to none in case of errors. Return None to preserve indexing for joining to df\n",
    "    street_number = None\n",
    "    street_name = None\n",
    "    locality = None\n",
    "    region1 = None\n",
    "    region2 = None\n",
    "    country = None\n",
    "    post_code = None\n",
    "    formatted_address = None\n",
    "    new_lat = None\n",
    "    new_lon = None\n",
    "    results = {}\n",
    "    \n",
    "    # Try to get the geocoded data from Google Maps API\n",
    "    try:\n",
    "        url = \"https://maps.googleapis.com/maps/api/geocode/json?\"\n",
    "        url += \"latlng=%s,%s&sensor=false&key=%s\" % (lat, lon, key)\n",
    "        v = urlopen(url).read()\n",
    "        j = json.loads(v)\n",
    "\n",
    "        if j['status'] == 'ZERO_RESULTS':\n",
    "            # pass to store None results\n",
    "            pass\n",
    "        else: \n",
    "            address_components = j['results'][0]    \n",
    "            # Check if individual address elements exist and update vars\n",
    "            for c in address_components['address_components']:\n",
    "                if \"street_number\" in c['types']:\n",
    "                    street_number = c['long_name']\n",
    "                if \"route\" in c['types']:\n",
    "                    street_name = c['long_name']\n",
    "                if \"locality\" in c['types']:\n",
    "                    locality = c['long_name']\n",
    "                if \"administrative_area_level_1\" in c['types']:\n",
    "                    region1 = c['long_name']            \n",
    "                if \"administrative_area_level_2\" in c['types']:\n",
    "                    region2 = c['long_name']                        \n",
    "                if \"country\" in c['types']:\n",
    "                    country = c['long_name']\n",
    "                if \"postal_code\" in c[\"types\"]:\n",
    "                    post_code = c['long_name']\n",
    "                    \n",
    "            # Capture formatted address \n",
    "            formatted_address = j['results'][0]['formatted_address']   \n",
    "\n",
    "            # Capture exact rooftop lat and lon used by Google Maps API\n",
    "            new_lat = j['results'][0]['geometry']['location']['lat']\n",
    "            new_lon = j['results'][0]['geometry']['location']['lng']\n",
    "\n",
    "        # Create dictionary of results\n",
    "        for i in ('street_number', \n",
    "                  'street_name', \n",
    "                  'locality', \n",
    "                  'region1', \n",
    "                  'region2', \n",
    "                  'country', \n",
    "                  'post_code', \n",
    "                  'formatted_address', \n",
    "                  'new_lat', \n",
    "                  'new_lon'):\n",
    "            results[i] = locals()[i]\n",
    "\n",
    "        return results\n",
    "    \n",
    "    # Set fields to None if any errors and return\n",
    "    except:\n",
    "        # Create dictionary of results\n",
    "        for i in ('street_number', \n",
    "                  'street_name', \n",
    "                  'locality', \n",
    "                  'region1', \n",
    "                  'region2', \n",
    "                  'country', \n",
    "                  'post_code', \n",
    "                  'formatted_address', \n",
    "                  'new_lat', \n",
    "                  'new_lon'):\n",
    "            results[i] = locals()[i]\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for dataframe\n",
    "col_names = ['street_number',\n",
    "             'street_name',\n",
    "             'locality',\n",
    "             'region1',\n",
    "             'region2',\n",
    "             'country',\n",
    "             'post_code',\n",
    "             'formatted_address',\n",
    "             'lat',\n",
    "             'lon']\n",
    "\n",
    "# Create an empty dataframe to store the results\n",
    "#geo_info = pd.DataFrame(columns=col_names)\n",
    "\n",
    "# Get geoinfo for each weather station and append to geo_info\n",
    "#for i in range(0,len(StationDetails)):\n",
    "#    results = getlocation(StationDetails.loc[i, 'Lat'], StationDetails.loc[i, 'Lon'])\n",
    "#    geo_info = geo_info.append(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save geoinfo results to feather for future use\n",
    "#feather.write_dataframe(geo_info, \"/Users/todddequincey/globalwarming/data/geo_info.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join the Google API weather station data to the original dataset\n",
    "StationDetails = StationDetails.join(geo_info, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with np.nan\n",
    "StationDetails = StationDetails.fillna(value=pd.np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the unique weather stations which have missing locality (city) or Country location information\n",
    "null_records = StationDetails[StationDetails['locality'].isnull() | \n",
    "                              StationDetails['country'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11433"
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of weather stations with missing data\n",
    "len(null_records['StationId'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountryId\n",
       "AR     109\n",
       "AS     362\n",
       "AY     204\n",
       "BR     515\n",
       "CA     846\n",
       "CH     398\n",
       "CI     130\n",
       "FI     231\n",
       "FR     218\n",
       "GL     103\n",
       "IN     272\n",
       "JA     182\n",
       "KZ     139\n",
       "RS    1212\n",
       "SF     148\n",
       "SW     230\n",
       "SZ     132\n",
       "UK     548\n",
       "US    1627\n",
       "Name: StationId, dtype: int64"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarise the country code of weather stations with missing data (more than 100 missing values)\n",
    "results = null_records.groupby('CountryId').count()\n",
    "results = results.loc[results['StationId'] >100,'StationId']\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert weather station id to int for all stations with missing location info\n",
    "# NOTE: To be removed from the weather station records if they exist\n",
    "stn_no_loc = null_records.StationId\n",
    "stn_no_loc = stn_no_loc.unique().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all weather stations with missing locality (city) or country location information from the dataset\n",
    "# Note: Should have done this BEFORE collecting the location data from the Google API\n",
    "StationDetails = StationDetails[StationDetails['locality'].notnull()] \n",
    "StationDetails = StationDetails[StationDetails['country'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index the df\n",
    "StationDetails = StationDetails.reset_index()\n",
    "#StationDetails = StationDetails.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "keep_cols = ['StationId', 'StationName', 'Elevation', 'StartDate', 'EndDate', 'locality', 'region1', 'country', 'lat', 'lon']\n",
    "StationDetails = StationDetails.loc[:,keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "StationDetails.columns = ['StationId', 'StationName', 'Elevation', 'StartDate', 'EndDate', 'City', 'State', 'Country', 'Lat', 'Lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to feather format\n",
    "feather.write_dataframe(StationDetails, \"/Users/todddequincey/globalwarming/data/StationDetails.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# StationRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Combine individual weather station files into single annual files\n",
    "- Consolidate the daily weather recordings into more meaningful monthly averages\n",
    "- Complete any required data cleaning \n",
    "- Load the data into the MySQL\n",
    "\n",
    "As the size of the monthly summarised data is expected to fit in memory, the data will be saved into a single dataframe before being imported into MySQL.\n",
    "\n",
    "If this is not the case, each year of data will be loaded into MySQL separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get the names of all files in the dir\n",
    "def create_annual_file(folder_path, year):\n",
    "    '''\n",
    "        For all of the CSV files in a given folder, combine all of the CSVs into a single annual file\n",
    "        \n",
    "        folder_path = String. The folder path where each of the annual folders are saved\n",
    "        year = String. The calendar year file being created.\n",
    "        \n",
    "        returns a DataFrame\n",
    "    '''\n",
    "    # Full path name\n",
    "    full_path = folder_path + year\n",
    "    \n",
    "    # Get file names in dir and convert to a list\n",
    "    file_names = list(os.listdir(full_path))\n",
    "    \n",
    "    # Calculate the number of files in the dir\n",
    "    no_files = len(file_names)\n",
    "    \n",
    "    # Combine the CSVs\n",
    "    combined_file = pd.concat([pd.read_csv(str(full_path + \"/\" + f), dtype={'STATION': int}) for f in file_names])\n",
    "    \n",
    "    # Return combined_csv df\n",
    "    return combined_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Clean the CSV\n",
    "def clean_files(df):\n",
    "    '''\n",
    "        Clean annual files\n",
    "    \n",
    "        df = dataframe returned from the combined_csv function. \n",
    "        \n",
    "        returns a DataFrame\n",
    "    '''\n",
    "    \n",
    "    # Create the station_records df \n",
    "    df = df.drop(labels = ['LATITUDE', \n",
    "                           'LONGITUDE', \n",
    "                           'NAME', \n",
    "                           'ELEVATION', \n",
    "                           'PRCP_ATTRIBUTES', \n",
    "                           'TEMP_ATTRIBUTES', \n",
    "                           'DEWP_ATTRIBUTES', \n",
    "                           'SLP_ATTRIBUTES', \n",
    "                           'STP_ATTRIBUTES', \n",
    "                           'VISIB_ATTRIBUTES', \n",
    "                           'WDSP_ATTRIBUTES', \n",
    "                           'MAX_ATTRIBUTES', \n",
    "                           'MIN_ATTRIBUTES', \n",
    "                           'PRCP_ATTRIBUTES', \n",
    "                           'FRSHTT'], \n",
    "                 axis=1)\n",
    "\n",
    "    # Replace all 999.9 and 9999.9 missing values with np.NaN\n",
    "    df = df.replace(999.9, np.NaN)\n",
    "    df = df.replace(9999.9, np.NaN)    \n",
    "    \n",
    "    # Rename the columns \n",
    "    df.columns = ['StationId', \n",
    "                  'Date', \n",
    "                  'Temp', \n",
    "                  'Dew', \n",
    "                  'SLP', \n",
    "                  'StationPressure', \n",
    "                  'Visib', \n",
    "                  'WindSpeed', \n",
    "                  'MaxWindSpeed', \n",
    "                  'Gust', \n",
    "                  'MaxTemp', \n",
    "                  'MinTemp', \n",
    "                  'Precip', \n",
    "                  'SnowDepth']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Calculate monthly averages, min and max\n",
    "def calc_monthly(df):\n",
    "    # Calculate and add the Year and Month columns to the df\n",
    "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
    "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
    "\n",
    "    # Drop the date column\n",
    "    df = df.drop(labels='Date', axis=1)\n",
    "    \n",
    "    # Group the rows by StationId, year and month \n",
    "    df_grouped = df.groupby(by=['StationId','Year','Month'])\n",
    "\n",
    "    # Calculate the avg, min and max fields and a field to count how many records are included in each calc\n",
    "    df_avg = df_grouped.mean()\n",
    "    df_min = df_grouped.min()\n",
    "    df_max = df_grouped.max()\n",
    "    df_count = df_grouped.size().to_frame()\n",
    "\n",
    "    # Left join df_max, df_min and df_count\n",
    "    df = df_avg.join(df_max.loc[:, ['MaxTemp', 'MinTemp']], rsuffix='_max')\n",
    "    df = df.join(df_min.loc[:, ['MaxTemp','MinTemp']], rsuffix='_min')\n",
    "    df = df.join(df_count)\n",
    "    \n",
    "    # Rename Min and Max temp column variants\n",
    "    col_names = list(df.columns)\n",
    "    col_names[8] = \"AvgMaxTemp\"\n",
    "    col_names[9] = \"AvgMinTemp\"\n",
    "    col_names[12] = \"MaxTemp\"\n",
    "    col_names[13] = \"MaxMinTemp\"\n",
    "    col_names[14] = \"MinMaxTemp\"\n",
    "    col_names[15] = \"MinTemp\"\n",
    "    col_names[16] = \"NumberDailyRecords\"\n",
    "    df.columns = col_names\n",
    "\n",
    "    # Reset the index of the df so it is no longer grouped\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run all functions\n",
    "def run(folder_path, start_year, end_year):\n",
    "    '''\n",
    "        Converts individual weather station records into annual summary/file/.\n",
    "        Cleans the annual summary.\n",
    "        Calculates monthly summary level data\n",
    "        \n",
    "        file_path = String. Path to where all of the folders for each year are saved.\n",
    "        start_year = Int. Start year to combine data\n",
    "        end_year = Int. End year to combine data\n",
    "        \n",
    "        returns pd.DataFrame\n",
    "    '''    \n",
    "    \n",
    "    # Dataframe to store the results\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    # Create range of years\n",
    "    years = range(start_year, end_year + 1)\n",
    "    \n",
    "    for year in years:\n",
    "        try:\n",
    "            # Create annual file\n",
    "            print(\"Creating annual file for {}...\".format(year))\n",
    "            df = create_annual_file(folder_path, str(year))\n",
    "            print(\"{} file created\".format(year))\n",
    "\n",
    "            # Clean annual file\n",
    "            print(\"Cleaning {} file...\".format(year))\n",
    "            df = clean_files(df)\n",
    "            print(\"{} file cleaned\".format(year))    \n",
    "\n",
    "            # Convert the data into monthly summary\n",
    "            print(\"Converting daily records in {} file into monthly summary...\".format(year))\n",
    "            df = calc_monthly(df)\n",
    "            print(\"{} converted into monthly summary\".format(year))        \n",
    "\n",
    "            # Append the results to the dataframe\n",
    "            results = results.append(df)\n",
    "            print(\"-----------------------\")\n",
    "        \n",
    "        # Catch all except clause for any errors\n",
    "        except Exception as e:\n",
    "            print(\"{} error with {}\".format(e, year))\n",
    "            print(\"-----------------------\")\n",
    "\n",
    "    # Reset the index\n",
    "    results = results.reset_index(drop=True)\n",
    "    \n",
    "    # Print success message\n",
    "    print(\"Finished loading data.\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Read and Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load Pre-Saved Data into Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load data from feather format (fastest read and write format)\n",
    "StationRecords = feather.read_dataframe(\"/Users/todddequincey/globalwarming/data/GSOD/1960-2019_monthly.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Read in the raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run all of the functions and save the df\n",
    "# StationRecords = run(\"/Users/todddequincey/Downloads/GSOD/\", 1960, 2019)\n",
    "\n",
    "# Round the decimals to three places\n",
    "#StationRecords = StationRecords.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating annual file for 1970...\n",
      "1970 file created\n",
      "Cleaning 1970 file...\n",
      "1970 file cleaned\n",
      "Converting daily records in 1970 file into monthly summary...\n",
      "1970 converted into monthly summary\n",
      "-----------------------\n",
      "Finished loading data.\n"
     ]
    }
   ],
   "source": [
    "test = run(\"/Users/todddequincey/Downloads/GSOD/\", 1970, 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "72224603844 in test.StationId.unique()\n",
    "#7222463844 in test.StationId.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Stations between Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Redundant Records\n",
    "Remove weather stations with no location information and/or set differences between StationDetails and StationRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove any weather station records where the weather station location information is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111147"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of records to be removed\n",
    "len(StationRecords[StationRecords.StationId.isin(stn_no_loc)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215967"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of records after removal \n",
    "len(StationRecords[~StationRecords.StationId.isin(stn_no_loc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all weather stations with no location infomation\n",
    "StationRecords = StationRecords[~StationRecords.StationId.isin(stn_no_loc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for further weather stations in StationRecords not in StationDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique stations in each dataset\n",
    "records = set(StationRecords.StationId.unique().astype(int))\n",
    "details = set(StationDetails.StationId.unique().astype(int))\n",
    "\n",
    "# Set difference\n",
    "missing_stations = records.difference(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 862,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of missing stations\n",
    "len(missing_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove weather stations from records dataset\n",
    "StationRecords = StationRecords[~StationRecords.StationId.isin(missing_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6389"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique weather stations which remain\n",
    "len(StationRecords.StationId.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for stations in StationDetails which are not in StationRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique stations in each dataset\n",
    "records = set(StationRecords.StationId.unique().astype(int))\n",
    "details = set(StationDetails.StationId.unique().astype(int))\n",
    "\n",
    "# Set difference\n",
    "missing_stations = details.difference(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11913"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of unused weather stations\n",
    "len(missing_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant weather station records from StationDetails\n",
    "StationDetails = StationDetails[~StationDetails.StationId.isin(missing_stations)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Save the data to feather format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save the dataframe to feather format (fastest to read and write for future use in Python)\n",
    "feather.write_dataframe(StationRecords, \"/Users/todddequincey/globalwarming/data/GSOD/1960-2019_monthly.feather\")\n",
    "feather.write_dataframe(StationDetails, \"/Users/todddequincey/globalwarming/data/StationDetails.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load Data to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection to the database\n",
    "engine = create_engine('mysql+pymysql://root:@localhost/globalwarming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StationDetails data into MySQL...\n",
      "StationDetails data loaded to MySQL\n"
     ]
    }
   ],
   "source": [
    "# Print progress message\n",
    "print(\"Loading StationDetails data into MySQL...\")\n",
    "\n",
    "# Export data into MySQL\n",
    "StationDetails.to_sql(con=engine,\n",
    "            name='StationDetails', \n",
    "            if_exists = 'append', \n",
    "            index=False)\n",
    "\n",
    "# Print success message\n",
    "print(\"StationDetails data loaded to MySQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StationRecords data into MySQL...\n",
      "StationRecords data loaded to MySQL\n"
     ]
    }
   ],
   "source": [
    "# Print progress message\n",
    "print(\"Loading StationRecords data into MySQL...\")\n",
    "\n",
    "# Export data into MySQL\n",
    "StationRecords.to_sql(con=engine,\n",
    "            name='StationRecords', \n",
    "            if_exists = 'append', \n",
    "            index=False)\n",
    "\n",
    "# Print success message\n",
    "print(\"StationRecords data loaded to MySQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
