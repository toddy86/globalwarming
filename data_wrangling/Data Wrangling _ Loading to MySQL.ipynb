{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import feather\n",
    "from urllib.request import urlopen\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clean the station details data\n",
    "- Enrich the data by adding the weather station location details (e.g. city, state, country etc)\n",
    "- Load the data into the MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_details = pd.read_csv(\"/Users/todddequincey/globalwarming/data/station_details.csv\")\n",
    "geo_info = feather.read_dataframe(\"/Users/todddequincey/globalwarming/data/geo_info.feather\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Geolocation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/todddequincey/Desktop/api_key.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-bd2e5814d0df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read in API key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/todddequincey/Desktop/api_key.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/todddequincey/Desktop/api_key.txt'"
     ]
    }
   ],
   "source": [
    "# Read in API key\n",
    "key = open(\"/Users/todddequincey/Desktop/api_key.txt\",\"r\")\n",
    "key = key.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the location of each weather station\n",
    "def getlocation(station_id, lat, lon):\n",
    "    # Set return vars to none in case of errors. Return None to preserve indexing for joining to df\n",
    "    station_id = station_id\n",
    "    street_number = None\n",
    "    street_name = None\n",
    "    locality = None\n",
    "    region1 = None\n",
    "    region2 = None\n",
    "    country = None\n",
    "    post_code = None\n",
    "    formatted_address = None\n",
    "    lat = lat\n",
    "    lon = lon\n",
    "    new_lat = None\n",
    "    new_lon = None\n",
    "    results = {}\n",
    "    \n",
    "    # Try to get the geocoded data from Google Maps API\n",
    "    try:\n",
    "        url = \"https://maps.googleapis.com/maps/api/geocode/json?\"\n",
    "        url += \"latlng=%s,%s&sensor=false&key=%s\" % (lat, lon, key)\n",
    "        v = urlopen(url).read()\n",
    "        j = json.loads(v)\n",
    "\n",
    "        if j['status'] == 'ZERO_RESULTS':\n",
    "            # pass to store None results\n",
    "            pass\n",
    "        else: \n",
    "            address_components = j['results'][0]    \n",
    "            # Check if individual address elements exist and update vars\n",
    "            for c in address_components['address_components']:\n",
    "                if \"street_number\" in c['types']:\n",
    "                    street_number = c['long_name']\n",
    "                if \"route\" in c['types']:\n",
    "                    street_name = c['long_name']\n",
    "                if \"locality\" in c['types']:\n",
    "                    locality = c['long_name']\n",
    "                if \"administrative_area_level_1\" in c['types']:\n",
    "                    region1 = c['long_name']            \n",
    "                if \"administrative_area_level_2\" in c['types']:\n",
    "                    region2 = c['long_name']                        \n",
    "                if \"country\" in c['types']:\n",
    "                    country = c['long_name']\n",
    "                if \"postal_code\" in c[\"types\"]:\n",
    "                    post_code = c['long_name']\n",
    "                    \n",
    "            # Capture formatted address \n",
    "            formatted_address = j['results'][0]['formatted_address']   \n",
    "\n",
    "            # Capture exact rooftop lat and lon used by Google Maps API\n",
    "            new_lat = j['results'][0]['geometry']['location']['lat']\n",
    "            new_lon = j['results'][0]['geometry']['location']['lng']\n",
    "\n",
    "        # Create dictionary of results\n",
    "        for i in ('station_id',\n",
    "                  'street_number', \n",
    "                  'street_name', \n",
    "                  'locality', \n",
    "                  'region1', \n",
    "                  'region2', \n",
    "                  'country', \n",
    "                  'post_code', \n",
    "                  'formatted_address', \n",
    "                  'lat', \n",
    "                  'lon',\n",
    "                  'new_lat', \n",
    "                  'new_lon'):\n",
    "            results[i] = locals()[i]\n",
    "\n",
    "        return results\n",
    "    \n",
    "    # Set fields to None if any errors and return\n",
    "    except:\n",
    "        # Create dictionary of results\n",
    "        for i in ('station_id',\n",
    "                  'street_number', \n",
    "                  'street_name', \n",
    "                  'locality', \n",
    "                  'region1', \n",
    "                  'region2', \n",
    "                  'country', \n",
    "                  'post_code', \n",
    "                  'formatted_address', \n",
    "                  'lat', \n",
    "                  'lon',\n",
    "                  'new_lat', \n",
    "                  'new_lon'):\n",
    "            results[i] = locals()[i]\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of weather stations at lat lon (0,0)\n",
    "len(station_details[((station_details['Lat'] == 0) & (station_details['Lon'] == 0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows where lat and lon are both 0 (missing data)\n",
    "station_details = station_details[~((station_details['Lat'] == 0) & (station_details['Lon'] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1205"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of weather stations with missing lat or lon (np.NaN)\n",
    "len(station_details[((station_details['Lat'].isnull()) | (station_details['Lon'].isnull()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows which missing lat and lon details (np.NaN)\n",
    "station_details = station_details[~((station_details['Lat'].isnull()) | (station_details['Lon'].isnull()))]\n",
    "station_details = station_details.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for dataframe\n",
    "col_names = ['station_id',\n",
    "             'street_number',\n",
    "             'street_name',\n",
    "             'locality',\n",
    "             'region1',\n",
    "             'region2',\n",
    "             'country',\n",
    "             'post_code',\n",
    "             'formatted_address',\n",
    "             'lat',\n",
    "             'lon',\n",
    "             'new_lat',\n",
    "             'new_lon']\n",
    "\n",
    "# Create an empty dataframe to store the results\n",
    "#geo_info = pd.DataFrame(columns=col_names)\n",
    "\n",
    "# Get geoinfo for each weather station and append to geo_info\n",
    "#for i in range(0,len(station_details)):\n",
    "#    results = getlocation(station_details.loc[i, 'StationId'], station_details.loc[i, 'Lat'], station_details.loc[i, 'Lon'])\n",
    "#    geo_info = geo_info.append(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save geoinfo results to feather for future use\n",
    "#feather.write_dataframe(geo_info, \"/Users/todddequincey/globalwarming/data/geo_info.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28151"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(station_details.StationId.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28151"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(geo_info.station_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join the Google API weather station data to the original dataset\n",
    "station_details = pd.concat([station_details, geo_info], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with np.nan\n",
    "station_details = station_details.fillna(value=pd.np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the unique weather stations which have missing locality (city) or Country location information\n",
    "null_records = station_details[station_details['locality'].isnull() | \n",
    "                              station_details['country'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9830"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of weather stations with missing data\n",
    "len(null_records['StationId'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountryId\n",
       "AR     125\n",
       "AY     199\n",
       "BR     918\n",
       "CA     605\n",
       "CH     102\n",
       "FI     134\n",
       "ID     162\n",
       "IT     176\n",
       "KZ     191\n",
       "NO     396\n",
       "RS    1068\n",
       "SW     440\n",
       "TU     169\n",
       "UK     578\n",
       "US     867\n",
       "Name: StationId, dtype: int64"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarise the country code of weather stations with missing data (more than 100 missing values)\n",
    "results = null_records.groupby('CountryId').count()\n",
    "results = results.loc[results['StationId'] >100,'StationId']\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert weather station id to int for all stations with missing location info\n",
    "# NOTE: To be removed from the weather station records if they exist\n",
    "stn_no_loc = null_records.StationId\n",
    "stn_no_loc = stn_no_loc.unique().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all weather stations with missing locality (city) or country location information from the dataset\n",
    "# Note: Should have done this BEFORE collecting the location data from the Google API\n",
    "station_details = station_details[station_details['locality'].notnull()] \n",
    "station_details = station_details[station_details['country'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index the df\n",
    "station_details = station_details.reset_index()\n",
    "#station_details = station_details.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "keep_cols = ['StationId', 'StationName', 'Elevation', 'StartDate', 'EndDate', 'locality', 'region1', 'country', 'new_lat', 'new_lon']\n",
    "station_details = station_details.loc[:,keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "station_details.columns = ['StationId', 'StationName', 'Elevation', 'StartDate', 'EndDate', 'City', 'State', 'Country', 'Lat', 'Lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to feather format\n",
    "feather.write_dataframe(station_details, \"/Users/todddequincey/globalwarming/data/station_details.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Station Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Combine individual weather station files into single annual files\n",
    "- Consolidate the daily weather recordings into more meaningful monthly averages\n",
    "- Complete any required data cleaning \n",
    "- Load the data into the MySQL\n",
    "\n",
    "As the size of the monthly summarised data is expected to fit in memory, the data will be saved into a single dataframe before being imported into MySQL.\n",
    "\n",
    "If this is not the case, each year of data will be loaded into MySQL separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get the names of all files in the dir\n",
    "def create_annual_file(folder_path, year):\n",
    "    '''\n",
    "        For all of the CSV files in a given folder, combine all of the CSVs into a single annual file\n",
    "        \n",
    "        folder_path = String. The folder path where each of the annual folders are saved\n",
    "        year = String. The calendar year file being created.\n",
    "        \n",
    "        returns a DataFrame\n",
    "    '''\n",
    "    # Full path name\n",
    "    full_path = folder_path + year\n",
    "    \n",
    "    # Get file names in dir and convert to a list\n",
    "    file_names = list(os.listdir(full_path))\n",
    "    \n",
    "    # Calculate the number of files in the dir\n",
    "    no_files = len(file_names)\n",
    "    \n",
    "    # Combine the CSVs\n",
    "    combined_file = pd.concat([pd.read_csv(str(full_path + \"/\" + f), dtype={'STATION': int}) for f in file_names])\n",
    "    \n",
    "    # Return combined_csv df\n",
    "    return combined_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Clean the CSV\n",
    "def clean_files(df):\n",
    "    '''\n",
    "        Clean annual files\n",
    "    \n",
    "        df = dataframe returned from the combined_csv function. \n",
    "        \n",
    "        returns a DataFrame\n",
    "    '''\n",
    "    \n",
    "    # Create the station_records df \n",
    "    df = df.drop(labels = ['LATITUDE', \n",
    "                           'LONGITUDE', \n",
    "                           'NAME', \n",
    "                           'ELEVATION', \n",
    "                           'PRCP_ATTRIBUTES', \n",
    "                           'TEMP_ATTRIBUTES', \n",
    "                           'DEWP_ATTRIBUTES', \n",
    "                           'SLP_ATTRIBUTES', \n",
    "                           'STP_ATTRIBUTES', \n",
    "                           'VISIB_ATTRIBUTES', \n",
    "                           'WDSP_ATTRIBUTES', \n",
    "                           'MAX_ATTRIBUTES', \n",
    "                           'MIN_ATTRIBUTES', \n",
    "                           'PRCP_ATTRIBUTES', \n",
    "                           'FRSHTT'], \n",
    "                 axis=1)\n",
    "\n",
    "    # Replace all 999.9 and 9999.9 missing values with np.NaN\n",
    "    df = df.replace(999.9, np.NaN)\n",
    "    df = df.replace(9999.9, np.NaN)    \n",
    "    \n",
    "    # Rename the columns \n",
    "    df.columns = ['StationId', \n",
    "                  'Date', \n",
    "                  'Temp', \n",
    "                  'Dew', \n",
    "                  'SLP', \n",
    "                  'StationPressure', \n",
    "                  'Visib', \n",
    "                  'WindSpeed', \n",
    "                  'MaxWindSpeed', \n",
    "                  'Gust', \n",
    "                  'MaxTemp', \n",
    "                  'MinTemp', \n",
    "                  'Precip', \n",
    "                  'SnowDepth']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Calculate monthly averages, min and max\n",
    "def calc_monthly(df):\n",
    "    # Calculate and add the Year and Month columns to the df\n",
    "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
    "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
    "\n",
    "    # Drop the date column\n",
    "    df = df.drop(labels='Date', axis=1)\n",
    "    \n",
    "    # Group the rows by StationId, year and month \n",
    "    df_grouped = df.groupby(by=['StationId','Year','Month'])\n",
    "\n",
    "    # Calculate the avg, min and max fields and a field to count how many records are included in each calc\n",
    "    df_avg = df_grouped.mean()\n",
    "    df_min = df_grouped.min()\n",
    "    df_max = df_grouped.max()\n",
    "    df_count = df_grouped.size().to_frame()\n",
    "\n",
    "    # Left join df_max, df_min and df_count\n",
    "    df = df_avg.join(df_max.loc[:, ['MaxTemp', 'MinTemp']], rsuffix='_max')\n",
    "    df = df.join(df_min.loc[:, ['MaxTemp','MinTemp']], rsuffix='_min')\n",
    "    df = df.join(df_count)\n",
    "    \n",
    "    # Rename Min and Max temp column variants\n",
    "    col_names = list(df.columns)\n",
    "    col_names[8] = \"AvgMaxTemp\"\n",
    "    col_names[9] = \"AvgMinTemp\"\n",
    "    col_names[12] = \"MaxTemp\"\n",
    "    col_names[13] = \"MaxMinTemp\"\n",
    "    col_names[14] = \"MinMaxTemp\"\n",
    "    col_names[15] = \"MinTemp\"\n",
    "    col_names[16] = \"NumberDailyRecords\"\n",
    "    df.columns = col_names\n",
    "\n",
    "    # Reset the index of the df so it is no longer grouped\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run all functions\n",
    "def run(folder_path, start_year, end_year):\n",
    "    '''\n",
    "        Converts individual weather station records into annual summary/file/.\n",
    "        Cleans the annual summary.\n",
    "        Calculates monthly summary level data\n",
    "        \n",
    "        file_path = String. Path to where all of the folders for each year are saved.\n",
    "        start_year = Int. Start year to combine data\n",
    "        end_year = Int. End year to combine data\n",
    "        \n",
    "        returns pd.DataFrame\n",
    "    '''    \n",
    "    \n",
    "    # Dataframe to store the results\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    # Create range of years\n",
    "    years = range(start_year, end_year + 1)\n",
    "    \n",
    "    for year in years:\n",
    "        try:\n",
    "            # Create annual file\n",
    "            print(\"Creating annual file for {}...\".format(year))\n",
    "            df = create_annual_file(folder_path, str(year))\n",
    "            print(\"{} file created\".format(year))\n",
    "\n",
    "            # Clean annual file\n",
    "            print(\"Cleaning {} file...\".format(year))\n",
    "            df = clean_files(df)\n",
    "            print(\"{} file cleaned\".format(year))    \n",
    "\n",
    "            # Convert the data into monthly summary\n",
    "            print(\"Converting daily records in {} file into monthly summary...\".format(year))\n",
    "            df = calc_monthly(df)\n",
    "            print(\"{} converted into monthly summary\".format(year))        \n",
    "\n",
    "            # Append the results to the dataframe\n",
    "            results = results.append(df)\n",
    "            print(\"-----------------------\")\n",
    "        \n",
    "        # Catch all except clause for any errors\n",
    "        except Exception as e:\n",
    "            print(\"{} error with {}\".format(e, year))\n",
    "            print(\"-----------------------\")\n",
    "\n",
    "    # Reset the index\n",
    "    results = results.reset_index(drop=True)\n",
    "    \n",
    "    # Print success message\n",
    "    print(\"Finished loading data.\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Read and Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load Pre-Saved Data into Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load data from feather format (fastest read and write format)\n",
    "station_records = feather.read_dataframe(\"/Users/todddequincey/globalwarming/data/GSOD/1960-2018_monthly_full.feather\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Read in the raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run all of the functions and save the df\n",
    "#station_records = run(\"/Users/todddequincey/globalwarming/data/GSOD/\", 2012, 2018)\n",
    "\n",
    "# Round the decimals to three places\n",
    "#station_records = station_records.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Cleaning \n",
    "Further cleaning is considered necessary to address the issues associated with high level of daily and/or monthly missing weather station records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Daily Records\n",
    "Any monthly with missing weather station records of 5% or more are to be removed from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5122512"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of records before data cleaning\n",
    "len(station_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of days in each month (leap year excluded for simplicity)\n",
    "months_31 = [1,3,5,7,8,10,12]\n",
    "months_30 = [4,6,9,11]\n",
    "months_28 = [2]\n",
    "\n",
    "# Set minimum number of daily records required in each month\n",
    "days_31 = int(round(31 * 0.95,0))\n",
    "days_30 = int(round(30 * 0.95,0))\n",
    "days_28 = int(round(28 * 0.95,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to ensure no more than 5% missing values in any monthly average\n",
    "station_records = station_records[\n",
    "    # Months with 31 days\n",
    "    ((station_records['Month'].isin(months_31)) & \n",
    "    (station_records['NumberDailyRecords'] >= days_31)) |\n",
    "\n",
    "    # Months with 30 days\n",
    "    ((station_records['Month'].isin(months_30)) & \n",
    "    (station_records['NumberDailyRecords'] >= days_30)) |\n",
    "    \n",
    "    # Months with 28 days\n",
    "    ((station_records['Month'].isin(months_28)) & \n",
    "    (station_records['NumberDailyRecords'] >= days_28))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3907240"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of remaining records\n",
    "len(station_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Monthly Records\n",
    "Any weather station with more than 5% of months which are missing are to be removed from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Threshold for number of months ()\n",
    "month_thres = int(round(12 * 0.95,0))\n",
    "month_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272835"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of months in each year for each weather station\n",
    "counter = station_records.groupby(['StationId', 'Year']).count().reset_index()\n",
    "\n",
    "# Remove redundant columns\n",
    "counter = counter[[\"StationId\", \"Year\", \"Month\"]]\n",
    "\n",
    "# Number of weather stations which meet the threshold\n",
    "len(counter[counter['Month'] >= month_thres])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the month counter to the df\n",
    "station_records = pd.merge(station_records, counter, how=\"left\", on=[\"StationId\", \"Year\"], suffixes=('','_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep records which meet the threshold\n",
    "station_records = station_records[station_records['Month_count'] >= month_thres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Month_count columns\n",
    "station_records = station_records.drop(columns = \"Month_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227527"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of weather stationId which meet the threshold\n",
    "len(station_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Station Intersection between Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate intersection of weather stations of station_details and station_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique stations in each dataset\n",
    "records = set(station_records.StationId.unique().astype(int))\n",
    "details = set(station_details.StationId.unique().astype(int))\n",
    "\n",
    "# Intersection of weather stations in both datasets\n",
    "intersection = records.intersection(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep weather stations in the intersection of the datasets\n",
    "station_records = station_records[station_records.StationId.isin(intersection)]\n",
    "station_details = station_details[station_details.StationId.isin(intersection)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12068"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique weather stations which remain\n",
    "len(station_records.StationId.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Save the data to feather format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save the dataframe to feather format (fastest to read and write for future use in Python)\n",
    "feather.write_dataframe(station_records, \"/Users/todddequincey/globalwarming/data/GSOD/1960-2018_monthly_cleaned.feather\")\n",
    "feather.write_dataframe(station_details, \"/Users/todddequincey/globalwarming/data/station_details_cleaned.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load Data to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection to the database\n",
    "engine = create_engine('mysql+pymysql://root:@localhost/globalwarming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading station_details data into MySQL...\n",
      "station_details data loaded to MySQL\n"
     ]
    }
   ],
   "source": [
    "# Print progress message\n",
    "print(\"Loading station_details data into MySQL...\")\n",
    "\n",
    "# Export data into MySQL\n",
    "station_details.to_sql(con=engine,\n",
    "            name='StationDetails', \n",
    "            if_exists = 'append', \n",
    "            index=False)\n",
    "\n",
    "# Print success message\n",
    "print(\"station_details data loaded to MySQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading station_records data into MySQL...\n",
      "station_records data loaded to MySQL\n"
     ]
    }
   ],
   "source": [
    "# Print progress message\n",
    "print(\"Loading station_records data into MySQL...\")\n",
    "\n",
    "# Export data into MySQL\n",
    "station_records.to_sql(con=engine,\n",
    "            name='StationRecords', \n",
    "            if_exists = 'append', \n",
    "            index=False)\n",
    "\n",
    "# Print success message\n",
    "print(\"station_records data loaded to MySQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Data\n",
    "Calculate the daily average for individual weather stations. Three year average based on the first three years in the decade (e.g. 1960, 1961, 1962). Calculated for the 60s, 70s and 80s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily averages\n",
    "def calc_daily_avg(df, year):\n",
    "    # Calculate and add the Month and Day columns to the df\n",
    "    df['Year'] = year\n",
    "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
    "    df['Day'] = pd.DatetimeIndex(df['Date']).day\n",
    "\n",
    "    # Drop the date column\n",
    "    df = df.drop(labels='Date', axis=1)\n",
    "\n",
    "    # Drop any rows for 29 Feb to avoid leap year issues\n",
    "    df = df[~((df['Month'] == 2) & (df['Day'] == 29))]\n",
    "    \n",
    "    # Group the rows by StationId, month and day\n",
    "    df_grouped = df.groupby(by=['StationId', 'Month', 'Day'])\n",
    "\n",
    "    # Calculate the avg \n",
    "    df = df_grouped.mean()\n",
    "\n",
    "    # Reset the index of the df so it is no longer grouped\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Re-construct the date for the average\n",
    "    df['Date'] = pd.to_datetime(df.loc[:,['Year', 'Month', 'Day']])\n",
    "\n",
    "    # Drop unneeded columns\n",
    "    df = df.drop(labels=['Year', \n",
    "                         'Month', \n",
    "                         'Day', \n",
    "                         'Dew', \n",
    "                         'SLP', \n",
    "                         'StationPressure', \n",
    "                         'Visib',\n",
    "                         'WindSpeed', \n",
    "                         'MaxWindSpeed', \n",
    "                         'Gust',\n",
    "                         'Precip',\n",
    "                         'SnowDepth'], \n",
    "                 axis=1)\n",
    "\n",
    "    # Reorder the columns\n",
    "    new_order = [0,4,1,2,3]\n",
    "    df = df[df.columns[new_order]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all functions\n",
    "def run_daily(folder_path, start_year, end_year):\n",
    "    '''\n",
    "        Converts individual weather station records into annual summary/file/.\n",
    "        Cleans the annual summary.\n",
    "        Calculates individual weather station daily average for the first three year (e.g. daily average of 1960, 1961 and 1962).\n",
    "        \n",
    "        file_path = String. Path to where all of the folders for each year are saved.\n",
    "        start_year = Int. Start year to combine data\n",
    "        end_year = Int. End year to combine data\n",
    "        \n",
    "        returns pd.DataFrame\n",
    "    '''    \n",
    "    \n",
    "    # Dataframe to store the results\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    # Create range of years\n",
    "    years = range(start_year, end_year + 1)\n",
    "    \n",
    "    for year in years:\n",
    "        try:\n",
    "            # Create annual file\n",
    "            print(\"Creating annual file for {}...\".format(year))\n",
    "            df = create_annual_file(folder_path, str(year))\n",
    "            print(\"{} file created\".format(year))\n",
    "\n",
    "            # Clean annual file\n",
    "            print(\"Cleaning {} file...\".format(year))\n",
    "            df = clean_files(df)\n",
    "            print(\"{} file cleaned\".format(year))    \n",
    "\n",
    "            # Append the results to the dataframe\n",
    "            results = results.append(df)\n",
    "            print(\"-----------------------\")\n",
    "        \n",
    "        # Catch all except clause for any errors\n",
    "        except Exception as e:\n",
    "            print(\"{} error with {}\".format(e, year))\n",
    "            print(\"-----------------------\")\n",
    "\n",
    "    # Reset the index\n",
    "    results = results.reset_index(drop=True)\n",
    "    \n",
    "    # Print success message\n",
    "    print(\"Finished loading data.\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating annual file for 1960...\n",
      "1960 file created\n",
      "Cleaning 1960 file...\n",
      "1960 file cleaned\n",
      "-----------------------\n",
      "Creating annual file for 1961...\n",
      "1961 file created\n",
      "Cleaning 1961 file...\n",
      "1961 file cleaned\n",
      "-----------------------\n",
      "Creating annual file for 1962...\n",
      "1962 file created\n",
      "Cleaning 1962 file...\n",
      "1962 file cleaned\n",
      "-----------------------\n",
      "Finished loading data.\n",
      "Creating annual file for 1970...\n",
      "1970 file created\n",
      "Cleaning 1970 file...\n",
      "1970 file cleaned\n",
      "-----------------------\n",
      "Creating annual file for 1971...\n",
      "1971 file created\n",
      "Cleaning 1971 file...\n",
      "1971 file cleaned\n",
      "-----------------------\n",
      "Creating annual file for 1972...\n",
      "1972 file created\n",
      "Cleaning 1972 file...\n",
      "1972 file cleaned\n",
      "-----------------------\n",
      "Finished loading data.\n",
      "Creating annual file for 1980...\n",
      "1980 file created\n",
      "Cleaning 1980 file...\n",
      "1980 file cleaned\n",
      "-----------------------\n",
      "Creating annual file for 1981...\n",
      "1981 file created\n",
      "Cleaning 1981 file...\n",
      "1981 file cleaned\n",
      "-----------------------\n",
      "Creating annual file for 1982...\n",
      "1982 file created\n",
      "Cleaning 1982 file...\n",
      "1982 file cleaned\n",
      "-----------------------\n",
      "Finished loading data.\n"
     ]
    }
   ],
   "source": [
    "# Read in data\n",
    "data_60 = run_daily(\"/Users/todddequincey/Downloads/\", 1960, 1962)\n",
    "data_70 = run_daily(\"/Users/todddequincey/Downloads/\", 1970, 1972)\n",
    "data_80 = run_daily(\"/Users/todddequincey/Downloads/\", 1980, 1982)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the daily averages\n",
    "df_60 = calc_daily_avg(data_60, 1960)\n",
    "df_70 = calc_daily_avg(data_70, 1970)\n",
    "df_80 = calc_daily_avg(data_80, 1980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into a single dataset\n",
    "daily_avg = pd.concat([df_60, df_70, df_80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round to 2 decimals\n",
    "daily_avg['Temp'] = round(daily_avg['Temp'], 2)\n",
    "daily_avg['MaxTemp'] = round(daily_avg['MaxTemp'],2)\n",
    "daily_avg['MinTemp'] = round(daily_avg['MinTemp'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to feather for future use\n",
    "#feather.write_dataframe(df_daily_avg, \"/Users/todddequincey/globalwarming/data/daily_avg.feather\")\n",
    "\n",
    "# Read in feather data\n",
    "daily_avg = feather.read_dataframe(\"/Users/todddequincey/globalwarming/data/daily_avg.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate intersection of weather stations of station_details and station_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique stations in each dataset\n",
    "details = set(station_details.StationId.unique().astype(int))\n",
    "daily = set(daily_avg.StationId.unique().astype(int))\n",
    "\n",
    "# Intersection of weather stations in both datasets\n",
    "intersection = details.intersection(daily)\n",
    "\n",
    "# Only keep weather stations in the intersection of the datasets\n",
    "daily_avg = daily_avg[daily_avg.StationId.isin(intersection)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection to the database\n",
    "engine = create_engine('mysql+pymysql://root:@localhost/globalwarming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading daily_avg data into MySQL...\n",
      "daily_avg data loaded to MySQL\n"
     ]
    }
   ],
   "source": [
    "# Print progress message\n",
    "print(\"Loading daily_avg data into MySQL...\")\n",
    "\n",
    "# Export data into MySQL\n",
    "daily_avg.to_sql(con=engine,\n",
    "            name='DailyAvg', \n",
    "            if_exists = 'append', \n",
    "            index=False)\n",
    "\n",
    "# Print success message\n",
    "print(\"daily_avg data loaded to MySQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
